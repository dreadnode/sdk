"""Tests for the Evaluation module."""

import pytest
from unittest.mock import AsyncMock, MagicMock, patch
import asyncio

from dreadnode.core.evaluations.evaluation import (
    Evaluation,
    evaluation,
    EvalWarning,
    current_dataset_row,
)
from dreadnode.core.evaluations.events import (
    EvalStart,
    EvalEnd,
    IterationStart,
    IterationEnd,
    ScenarioStart,
    ScenarioEnd,
    SampleComplete,
)
from dreadnode.core.evaluations.result import EvalResult, IterationResult, ScenarioResult
from dreadnode.core.evaluations.sample import Sample
from dreadnode.core.task import Task, task
from dreadnode.core.scorer import Scorer
from dreadnode.core.tracing.span import get_default_tracer


# ==============================================================================
# Fixtures
# ==============================================================================


@pytest.fixture
def tracer():
    """Get default tracer for tests."""
    return get_default_tracer()


@pytest.fixture
def simple_async_task(tracer):
    """A simple async task for testing."""

    @task
    async def my_task(x: int) -> int:
        return x * 2

    return my_task


@pytest.fixture
def simple_dataset():
    """A simple dataset for testing."""
    return [{"x": 1}, {"x": 2}, {"x": 3}]


@pytest.fixture
def simple_evaluation(simple_async_task, simple_dataset):
    """A simple evaluation for testing."""
    return Evaluation(
        name="test_eval",
        task=simple_async_task,
        dataset=simple_dataset,
    )


@pytest.fixture
def simple_scorer():
    """A simple scorer that returns value / 10."""

    @Scorer
    async def score(value: int) -> float:
        return min(value / 10.0, 1.0)

    return score


# ==============================================================================
# Evaluation Creation Tests
# ==============================================================================


class TestEvaluationCreation:
    """Tests for Evaluation creation."""

    def test_create_basic(self, simple_async_task, simple_dataset):
        """Test creating a basic evaluation."""
        eval_ = Evaluation(
            name="test",
            task=simple_async_task,
            dataset=simple_dataset,
        )

        assert eval_.name == "test"
        assert eval_.task is simple_async_task
        assert eval_.dataset == simple_dataset

    def test_create_with_name_autogenerated(self, simple_async_task, simple_dataset):
        """Test that name is autogenerated from task name."""
        eval_ = Evaluation(
            name="",
            task=simple_async_task,
            dataset=simple_dataset,
        )

        # Name should be "Eval <task_name>"
        assert "Eval" in eval_.name
        assert "my_task" in eval_.name

    def test_create_without_dataset_raises(self, simple_async_task):
        """Test that creating without dataset raises ValueError."""
        with pytest.raises(ValueError, match="must be provided"):
            Evaluation(name="test", task=simple_async_task)

    def test_create_with_iterations(self, simple_async_task, simple_dataset):
        """Test creating with iterations."""
        eval_ = Evaluation(
            name="test",
            task=simple_async_task,
            dataset=simple_dataset,
            iterations=5,
        )

        assert eval_.iterations == 5

    def test_create_with_concurrency(self, simple_async_task, simple_dataset):
        """Test creating with concurrency."""
        eval_ = Evaluation(
            name="test",
            task=simple_async_task,
            dataset=simple_dataset,
            concurrency=10,
        )

        assert eval_.concurrency == 10

    def test_create_with_scorers(self, simple_async_task, simple_dataset, simple_scorer):
        """Test creating with scorers."""
        eval_ = Evaluation(
            name="test",
            task=simple_async_task,
            dataset=simple_dataset,
            scorers=[simple_scorer],
        )

        assert len(Scorer.fit_many(eval_.scorers)) == 1

    def test_create_with_assert_scores(
        self, simple_async_task, simple_dataset, simple_scorer
    ):
        """Test creating with assert_scores."""
        eval_ = Evaluation(
            name="test",
            task=simple_async_task,
            dataset=simple_dataset,
            scorers=[simple_scorer],
            assert_scores=["score"],
        )

        assert eval_.assert_scores == ["score"]

    def test_create_with_parameters(self, simple_async_task, simple_dataset):
        """Test creating with parameter space."""
        eval_ = Evaluation(
            name="test",
            task=simple_async_task,
            dataset=simple_dataset,
            parameters={"factor": [1, 2, 3]},
        )

        assert eval_.parameters == {"factor": [1, 2, 3]}

    def test_create_with_tags(self, simple_async_task, simple_dataset):
        """Test creating with tags."""
        eval_ = Evaluation(
            name="test",
            task=simple_async_task,
            dataset=simple_dataset,
            tags=["custom_tag"],
        )

        assert "custom_tag" in eval_.tags

    def test_create_with_description(self, simple_async_task, simple_dataset):
        """Test creating with description."""
        eval_ = Evaluation(
            name="test",
            task=simple_async_task,
            dataset=simple_dataset,
            description="Test description",
        )

        assert eval_.description == "Test description"

    def test_create_with_max_errors(self, simple_async_task, simple_dataset):
        """Test creating with max_errors."""
        eval_ = Evaluation(
            name="test",
            task=simple_async_task,
            dataset=simple_dataset,
            max_errors=5,
        )

        assert eval_.max_errors == 5

    def test_create_with_max_consecutive_errors(self, simple_async_task, simple_dataset):
        """Test creating with max_consecutive_errors."""
        eval_ = Evaluation(
            name="test",
            task=simple_async_task,
            dataset=simple_dataset,
            max_consecutive_errors=3,
        )

        assert eval_.max_consecutive_errors == 3

    def test_create_with_dataset_input_mapping_list(
        self, simple_async_task, simple_dataset
    ):
        """Test creating with dataset_input_mapping as list."""
        eval_ = Evaluation(
            name="test",
            task=simple_async_task,
            dataset=simple_dataset,
            dataset_input_mapping=["x"],
        )

        assert eval_.dataset_input_mapping == ["x"]

    def test_create_with_dataset_input_mapping_dict(
        self, simple_async_task, simple_dataset
    ):
        """Test creating with dataset_input_mapping as dict."""
        eval_ = Evaluation(
            name="test",
            task=simple_async_task,
            dataset=simple_dataset,
            dataset_input_mapping={"x": "input_x"},
        )

        assert eval_.dataset_input_mapping == {"x": "input_x"}


# ==============================================================================
# Evaluation Properties Tests
# ==============================================================================


class TestEvaluationProperties:
    """Tests for Evaluation properties."""

    def test_repr(self, simple_evaluation):
        """Test __repr__ method."""
        result = repr(simple_evaluation)

        assert "Evaluation(" in result
        assert "name='test_eval'" in result

    def test_repr_with_parameters(self, simple_async_task, simple_dataset):
        """Test __repr__ with parameters."""
        eval_ = Evaluation(
            name="test",
            task=simple_async_task,
            dataset=simple_dataset,
            parameters={"factor": [1, 2]},
        )

        result = repr(eval_)

        assert "parameter_space=" in result

    def test_task_name_property(self, simple_evaluation):
        """Test task_name property."""
        assert simple_evaluation.task_name == "my_task"


# ==============================================================================
# Evaluation with_ Tests
# ==============================================================================


class TestEvaluationWith:
    """Tests for Evaluation.with_() method."""

    @pytest.mark.xfail(reason="Bug: with_() uses name_ instead of name in updates dict")
    def test_with_name(self, simple_evaluation):
        """Test with_ changing name."""
        result = simple_evaluation.with_(name="new_name")

        # The Evaluation model uses name, with_ maps it internally
        assert result.name == "new_name"

    def test_with_iterations(self, simple_evaluation):
        """Test with_ changing iterations."""
        result = simple_evaluation.with_(iterations=10)

        assert result.iterations == 10

    def test_with_concurrency(self, simple_evaluation):
        """Test with_ changing concurrency."""
        result = simple_evaluation.with_(concurrency=20)

        assert result.concurrency == 20

    def test_with_scorers(self, simple_evaluation, simple_scorer):
        """Test with_ adding scorers."""
        result = simple_evaluation.with_(scorers=[simple_scorer])

        assert len(Scorer.fit_many(result.scorers)) == 1

    def test_with_tags_replace(self, simple_evaluation):
        """Test with_ replacing tags."""
        result = simple_evaluation.with_(tags=["new_tag"])

        assert result.tags == ["new_tag"]

    def test_with_tags_append(self, simple_evaluation):
        """Test with_ appending tags."""
        result = simple_evaluation.with_(tags=["new_tag"], append=True)

        assert "new_tag" in result.tags
        assert "eval" in result.tags


# ==============================================================================
# Evaluation Decorator Tests
# ==============================================================================


class TestEvaluationDecorator:
    """Tests for the @evaluation decorator."""

    def test_decorator_with_dataset(self):
        """Test @evaluation decorator with dataset."""

        @evaluation(dataset=[{"x": 1}, {"x": 2}], name="test")
        async def my_task(x: int) -> int:
            return x * 2

        assert isinstance(my_task, Evaluation)
        assert my_task.name == "test"

    def test_decorator_with_options(self):
        """Test @evaluation decorator with all options."""

        @evaluation(
            dataset=[{"x": 1}],
            name="test",
            description="Test description",
            concurrency=5,
            iterations=3,
        )
        async def my_task(x: int) -> int:
            return x * 2

        assert my_task.description == "Test description"
        assert my_task.concurrency == 5
        assert my_task.iterations == 3


# ==============================================================================
# Evaluation Execution Tests
# ==============================================================================


class TestEvaluationExecution:
    """Tests for Evaluation execution."""

    @pytest.mark.asyncio
    async def test_run_basic(self, simple_evaluation):
        """Test basic evaluation run."""
        result = await simple_evaluation.run()

        assert isinstance(result, EvalResult)
        assert len(result.scenarios) == 1
        assert result.stop_reason == "finished"

    @pytest.mark.asyncio
    async def test_run_with_iterations(self, simple_async_task, simple_dataset):
        """Test evaluation run with multiple iterations."""
        eval_ = Evaluation(
            name="test",
            task=simple_async_task,
            dataset=simple_dataset,
            iterations=2,
        )

        result = await eval_.run()

        # Should have 2 iterations in the scenario
        assert len(result.scenarios[0].iterations) == 2

    @pytest.mark.asyncio
    async def test_stream_yields_events(self, simple_evaluation):
        """Test that stream yields proper events."""
        events = []
        async with simple_evaluation.stream() as stream:
            async for event in stream:
                events.append(event)

        event_types = [type(e).__name__ for e in events]

        assert "EvalStart" in event_types
        assert "ScenarioStart" in event_types
        assert "IterationStart" in event_types
        assert "SampleComplete" in event_types
        assert "IterationEnd" in event_types
        assert "ScenarioEnd" in event_types
        assert "EvalEnd" in event_types

    @pytest.mark.asyncio
    async def test_run_with_parameters(self, simple_async_task):
        """Test evaluation with parameter space."""

        @task
        async def configurable_task(x: int, *, factor: int = 1) -> int:
            return x * factor

        eval_ = Evaluation(
            name="test",
            task=configurable_task,
            dataset=[{"x": 1}],
            parameters={"factor": [2, 3]},
        )

        result = await eval_.run()

        # Should have 2 scenarios (one for each factor value)
        assert len(result.scenarios) == 2


# ==============================================================================
# Evaluation Result Tests
# ==============================================================================


class TestEvalResult:
    """Tests for EvalResult class."""

    def test_empty_result(self):
        """Test empty result."""
        result = EvalResult(scenarios=[])

        assert result.passed_count == 0
        assert result.failed_count == 0
        assert result.error_count == 0
        assert len(result.samples) == 0

    @pytest.mark.asyncio
    async def test_result_counts(self, simple_evaluation):
        """Test result counts after run."""
        result = await simple_evaluation.run()

        # All samples should pass (simple multiplication)
        assert result.passed_count == 3
        assert result.failed_count == 0
        assert result.error_count == 0
        assert result.pass_rate == 1.0

    @pytest.mark.asyncio
    async def test_result_samples(self, simple_evaluation):
        """Test accessing samples from result."""
        result = await simple_evaluation.run()

        assert len(result.samples) == 3
        assert all(isinstance(s, Sample) for s in result.samples)


# ==============================================================================
# ScenarioResult Tests
# ==============================================================================


class TestScenarioResult:
    """Tests for ScenarioResult class."""

    def test_empty_scenario(self):
        """Test empty scenario result."""
        result = ScenarioResult(params={})

        assert result.passed_count == 0
        assert result.failed_count == 0

    def test_scenario_with_params(self):
        """Test scenario with params."""
        result = ScenarioResult(params={"factor": 2})

        assert result.params == {"factor": 2}


# ==============================================================================
# IterationResult Tests
# ==============================================================================


class TestIterationResult:
    """Tests for IterationResult class."""

    def test_empty_iteration(self):
        """Test empty iteration result."""
        result = IterationResult(iteration=1)

        assert result.iteration == 1
        assert len(result.samples) == 0


# ==============================================================================
# Sample Tests
# ==============================================================================


class TestSample:
    """Tests for Sample class."""

    def test_basic_creation(self):
        """Test creating a basic sample."""
        sample = Sample(
            input=5,
            output=10,
            passed=True,
        )

        assert sample.input == 5
        assert sample.output == 10
        assert sample.passed is True
        assert sample.failed is False

    def test_with_error(self):
        """Test sample with error."""
        error = ValueError("Something went wrong")
        sample = Sample(
            input=5,
            output=None,
            error=error,
        )

        assert sample.error is error
        assert sample.failed is True

    def test_with_context(self):
        """Test sample with context."""
        sample = Sample(
            input=5,
            output=10,
            context={"extra": "info"},
        )

        assert sample.context == {"extra": "info"}


# ==============================================================================
# Event Tests
# ==============================================================================


class TestEvaluationEvents:
    """Tests for evaluation events."""

    def test_eval_start(self, simple_evaluation):
        """Test EvalStart event."""
        event = EvalStart(
            evaluation=simple_evaluation,
            dataset_size=3,
            scenario_count=1,
            total_iterations=1,
            total_samples=3,
        )

        assert event.evaluation is simple_evaluation
        assert event.dataset_size == 3
        assert event.total_samples == 3

    def test_sample_complete(self, simple_evaluation):
        """Test SampleComplete event."""
        sample = Sample(input=1, output=2, passed=True)
        event = SampleComplete(
            evaluation=simple_evaluation,
            run_id="run_123",
            sample=sample,
        )

        assert event.sample is sample
        assert event.run_id == "run_123"


# ==============================================================================
# Parameter Combinations Tests
# ==============================================================================


class TestParameterCombinations:
    """Tests for parameter combinations."""

    def test_no_parameters(self, simple_evaluation):
        """Test with no parameters returns single empty combo."""
        combos = simple_evaluation._get_param_combinations()

        assert combos == [{}]

    def test_single_parameter(self, simple_async_task, simple_dataset):
        """Test with single parameter."""
        eval_ = Evaluation(
            name="test",
            task=simple_async_task,
            dataset=simple_dataset,
            parameters={"a": [1, 2, 3]},
        )

        combos = eval_._get_param_combinations()

        assert len(combos) == 3
        assert {"a": 1} in combos
        assert {"a": 2} in combos
        assert {"a": 3} in combos

    def test_multiple_parameters(self, simple_async_task, simple_dataset):
        """Test with multiple parameters creates cartesian product."""
        eval_ = Evaluation(
            name="test",
            task=simple_async_task,
            dataset=simple_dataset,
            parameters={"a": [1, 2], "b": [10, 20]},
        )

        combos = eval_._get_param_combinations()

        assert len(combos) == 4  # 2 * 2
        assert {"a": 1, "b": 10} in combos
        assert {"a": 1, "b": 20} in combos
        assert {"a": 2, "b": 10} in combos
        assert {"a": 2, "b": 20} in combos


# ==============================================================================
# Context Variable Tests
# ==============================================================================


class TestCurrentDatasetRow:
    """Tests for current_dataset_row context variable."""

    def test_default_is_none(self):
        """Test default value is None."""
        assert current_dataset_row.get() is None

    @pytest.mark.asyncio
    async def test_set_and_get(self):
        """Test setting and getting the context variable."""
        row = {"x": 1, "y": 2}
        token = current_dataset_row.set(row)

        try:
            assert current_dataset_row.get() == row
        finally:
            current_dataset_row.reset(token)

        assert current_dataset_row.get() is None


# ==============================================================================
# Trace Context Tests
# ==============================================================================


class TestEvaluationTraceContext:
    """Tests for Evaluation trace context."""

    def test_get_trace_context(self, simple_evaluation):
        """Test _get_trace_context method."""
        ctx = simple_evaluation._get_trace_context()

        assert ctx.span_type == "evaluation"
        assert "task_name" in ctx.inputs
        assert "iterations" in ctx.params

    def test_should_trace_default(self, simple_evaluation):
        """Test _should_trace returns True by default."""
        assert simple_evaluation._should_trace() is True

    def test_should_trace_disabled(self, simple_async_task, simple_dataset):
        """Test _should_trace when trace=False."""
        eval_ = Evaluation(
            name="test",
            task=simple_async_task,
            dataset=simple_dataset,
            trace=False,
        )

        assert eval_._should_trace() is False
