---
title: dreadnode.optimization
---

{/*
::: dreadnode.optimization.study
::: dreadnode.optimization.trial
::: dreadnode.optimization.events
::: dreadnode.optimization.search
*/}

Direction
---------

```python
Direction = Literal['maximize', 'minimize']
```

The direction of optimization for the objective score.

ObjectivesLike
--------------

```python
ObjectivesLike = (
    Sequence[ScorerLike[OutputT] | str]
    | Mapping[str, ScorerLike[OutputT]]
)
```

The objectives to optimize for.

Study
-----

### concurrency

```python
concurrency: int = Config(default=1, ge=1)
```

The maximum number of trials to evaluate in parallel.

### constraints

```python
constraints: ScorersLike[CandidateT] | None = Config(
    default=None
)
```

A list of Scorer-like constraints to apply to candidates. If any constraint scores to a falsy value, the candidate is pruned.

### dataset

```python
dataset: (
    InputDataset[Any] | list[AnyDict] | FilePath | None
) = Config(default=None, expose_as=FilePath | None)
```

The dataset to use for the evaluation. Can be a list of inputs or a file path to load inputs from.
If `None`, an empty dataset with a single empty input will be used - in other words the task will
simply be evaluated once per trial.

This dataset will be used to evaluate each trial's task during scoring - the mean average score
for all metrics will be used as the trial's singular objective scores.

### description

```python
description: str = ''
```

A brief description of the study's purpose.

### directions

```python
directions: list[Direction] = Config(
    default_factory=lambda: ["maximize"]
)
```

The directions of optimization for the objective score.

The length must match the number of objectives.

### max\_trials

```python
max_trials: int = Config(default=100, ge=1)
```

The maximum number of trials to evaluate.

### name\_

```python
name_: str | None = Field(
    default=None, repr=False, exclude=False, alias="name"
)
```

The name of the study - otherwise derived from the objective.

### objectives

```python
objectives: Annotated[
    ObjectivesLike[OutputT], Config(expose_as=None)
]
```

The objectives to optimize for.

Can be a list/dict of scorer-like callables or string names of scorers already on the task.

### search\_strategy

```python
search_strategy: SkipValidation[Search[CandidateT]]
```

The search strategy to use for suggesting new trials.

### stop\_conditions

```python
stop_conditions: list[StudyStopCondition[CandidateT]] = (
    Config(default_factory=list)
)
```

A list of conditions that, if any are met, will stop the study.

### tags

```python
tags: list[str] = Config(default_factory=lambda: ['study'])
```

A list of tags associated with the study for logging.

### task\_factory

```python
task_factory: Callable[[CandidateT], Task[..., OutputT]]
```

A function that accepts a candidate and returns a configured Task ready for evaluation.

### clone

```python
clone() -> te.Self
```

Clone a task.

**Returns:**

* `Self`
  –A new Task instance with the same attributes as this one.

<Accordion title="Source code in dreadnode/optimization/study.py" icon="code">
```python
def clone(self) -> te.Self:
    """
    Clone a task.

    Returns:
        A new Task instance with the same attributes as this one.
    """
    return self.model_copy(deep=True)
```


</Accordion>

### console

```python
console() -> StudyResult[CandidateT]
```

Runs the optimization study with a live progress dashboard in the console.

<Accordion title="Source code in dreadnode/optimization/study.py" icon="code">
```python
async def console(self) -> StudyResult[CandidateT]:
    """Runs the optimization study with a live progress dashboard in the console."""

    adapter = StudyConsoleAdapter(self)
    return await adapter.run()
```


</Accordion>

### run

```python
run() -> StudyResult[CandidateT]
```

Execute the optimization study to completion and return final results.

This is a convenience method that runs the full optimization process and
returns only the final StudyEnd event containing the complete results.
Use this when you want the final results without processing intermediate events.

For real-time monitoring of the optimization process, use the stream() method instead.

**Raises:**

* `RuntimeError`
  –If the evaluation fails to complete properly.

<Accordion title="Source code in dreadnode/optimization/study.py" icon="code">
```python
async def run(self) -> StudyResult[CandidateT]:
    """
    Execute the optimization study to completion and return final results.

    This is a convenience method that runs the full optimization process and
    returns only the final StudyEnd event containing the complete results.
    Use this when you want the final results without processing intermediate events.

    For real-time monitoring of the optimization process, use the stream() method instead.

    Raises:
        RuntimeError: If the evaluation fails to complete properly.
    """
    async with self.stream() as stream:
        async for event in stream:
            if isinstance(event, StudyEnd):
                return event.result

    raise RuntimeError("Evaluation failed to complete")
```


</Accordion>

### stream

```python
stream() -> t.AsyncIterator[
    t.AsyncGenerator[StudyEvent[CandidateT], None]
]
```

Create an async context manager for the optimization event stream.

This provides a safe way to access the optimization event stream with proper
resource cleanup. The context manager ensures the async generator is properly
closed even if an exception occurs during iteration.

Usage

async with study.stream() as event\_stream:
async for event in event\_stream:
# Process optimization events
pass

**Yields:**

* `AsyncIterator[AsyncGenerator[StudyEvent[CandidateT], None]]`
  –An async generator that produces StudyEvent objects throughout the optimization.

<Accordion title="Source code in dreadnode/optimization/study.py" icon="code">
```python
@contextlib.asynccontextmanager
async def stream(
    self,
) -> t.AsyncIterator[t.AsyncGenerator[StudyEvent[CandidateT], None]]:
    """
    Create an async context manager for the optimization event stream.

    This provides a safe way to access the optimization event stream with proper
    resource cleanup. The context manager ensures the async generator is properly
    closed even if an exception occurs during iteration.

    Usage:
        async with study.stream() as event_stream:
            async for event in event_stream:
                # Process optimization events
                pass

    Yields:
        An async generator that produces StudyEvent objects throughout the optimization.
    """
    async with contextlib.aclosing(self._stream_traced()) as gen:
        yield gen
```


</Accordion>

### with\_

```python
with_(
    *,
    name: str | None = None,
    description: str | None = None,
    tags: list[str] | None = None,
    search_strategy: Search[CandidateT] | None = None,
    task_factory: Callable[[CandidateT], Task[..., OutputT]]
    | None = None,
    objectives: ObjectivesLike[OutputT] | None = None,
    directions: list[Direction] | None = None,
    dataset: InputDataset[Any]
    | list[AnyDict]
    | FilePath
    | None = None,
    concurrency: int | None = None,
    constraints: ScorersLike[CandidateT] | None = None,
    max_trials: int | None = None,
    stop_conditions: list[StudyStopCondition[CandidateT]]
    | None = None,
    append: bool = False,
) -> te.Self
```

Clone the study and modify its attributes.

**Returns:**

* `Self`
  –A new Study instance with the modified attributes.

<Accordion title="Source code in dreadnode/optimization/study.py" icon="code">
```python
def with_(
    self,
    *,
    name: str | None = None,
    description: str | None = None,
    tags: list[str] | None = None,
    search_strategy: Search[CandidateT] | None = None,
    task_factory: t.Callable[[CandidateT], Task[..., OutputT]] | None = None,
    objectives: ObjectivesLike[OutputT] | None = None,
    directions: list[Direction] | None = None,
    dataset: InputDataset[t.Any] | list[AnyDict] | FilePath | None = None,
    concurrency: int | None = None,
    constraints: ScorersLike[CandidateT] | None = None,
    max_trials: int | None = None,
    stop_conditions: list[StudyStopCondition[CandidateT]] | None = None,
    append: bool = False,
) -> te.Self:
    """
    Clone the study and modify its attributes.

    Returns:
        A new Study instance with the modified attributes.
    """
    new = self.clone()

    new.name_ = name or new.name
    new.description = description or new.description
    new.search_strategy = search_strategy or new.search_strategy
    new.task_factory = task_factory or new.task_factory
    new.dataset = dataset if dataset is not None else new.dataset
    new.concurrency = concurrency or new.concurrency
    new.max_trials = max_trials or new.max_trials

    new_tags = tags or []
    new_objectives = fit_objectives(objectives) if objectives is not None else []
    new_directions = directions or ["maximize"] * len(new_objectives)
    new_stop_conditions = stop_conditions or []
    new_constraints = Scorer.fit_many(constraints) if constraints is not None else []

    if len(new_directions) != len(new_objectives):
        raise ValueError(
            f"The number of directions ({len(new_directions)}) must match the "
            f"number of objectives ({len(new_objectives)})."
        )

    if append:
        new.tags = [*new.tags, *new_tags]
        new.objectives = [*fit_objectives(new.objectives), *new_objectives]
        new.directions = [*new.directions, *new_directions]
        new.stop_conditions = [*new.stop_conditions, *new_stop_conditions]
        new.constraints = [*Scorer.fit_many(new.constraints), *new_constraints]
    else:
        new.tags = new_tags or new.tags
        new.objectives = new_objectives or new.objectives
        new.directions = new_directions or new.directions
        new.stop_conditions = new_stop_conditions or new.stop_conditions
        new.constraints = new_constraints or new.constraints

    return new
```


</Accordion>
Trial
-----

Represents a single, evaluated point in the search space.

### all\_scores

```python
all_scores: dict[str, float]
```

A dictionary of all named metric mean values from the evaluation result.

This includes scores not directly related to the objective.

### candidate

```python
candidate: CandidateT
```

The candidate configuration being assessed.

### created\_at

```python
created_at: datetime
```

The creation timestamp of the trial, extracted from its ULID.

### error

```python
error: str | None = None
```

Any error which occurred while processing this trial.

### eval\_result

```python
eval_result: EvalResult | None = None
```

Complete evaluation result if the candidate was assessable by the evaluation engine.

### id

```python
id: ULID = Field(default_factory=ULID)
```

Unique identifier for the trial.

### output

```python
output: Any | None
```

Get the output of the trial.

### parent\_id

```python
parent_id: ULID | None = None
```

The id of the parent trial, used to reconstruct the search graph.

### pruning\_reason

```python
pruning_reason: str | None = None
```

Reason for pruning this trial, if applicable.

### score

```python
score: float = -float('inf')
```

The primary, single-value fitness score for this trial.
This is an average of all objective scores for this trial (higher is better).

### scores

```python
scores: dict[str, float] = {}
```

A dictionary of all named objective scores for this trial, adjusted
for the optimization direction (higher is better).

### status

```python
status: TrialStatus = 'pending'
```

Current status of the trial.

### step

```python
step: int = Field(default=0, init=False)
```

The optimization step which produced this trial.

### \_\_await\_\_

```python
__await__() -> t.Generator[t.Any, None, Trial[CandidateT]]
```

Await the completion of the trial.

<Accordion title="Source code in dreadnode/optimization/trial.py" icon="code">
```python
def __await__(self) -> t.Generator[t.Any, None, "Trial[CandidateT]"]:
    """
    Await the completion of the trial.
    """
    return self._future.__await__()
```


</Accordion>

### done

```python
done() -> bool
```

A non-blocking check to see if the trial's evaluation is complete.

<Accordion title="Source code in dreadnode/optimization/trial.py" icon="code">
```python
def done(self) -> bool:
    """A non-blocking check to see if the trial's evaluation is complete."""
    return self._future.done()
```


</Accordion>

### get\_score

```python
get_score(
    name: str | None = None, default: float = -float("inf")
) -> float
```

Get a specific named objective score, or the overall score if no name is given.

**Parameters:**

* **`name`**
  (`str | None`, default:
  `None`
  )
  –The name of the objective.
* **`default`**
  (`float`, default:
  `-float('inf')`
  )
  –The value to return if the named score is not found.

<Accordion title="Source code in dreadnode/optimization/trial.py" icon="code">
```python
def get_score(self, name: str | None = None, default: float = -float("inf")) -> float:
    """
    Get a specific named objective score, or the overall score if no name is given.

    Args:
        name: The name of the objective.
        default: The value to return if the named score is not found.
    """
    if name is not None:
        return self.scores.get(name, default)
    return self.score
```


</Accordion>

### wait\_for

```python
wait_for(
    *trials: Trial[CandidateT],
) -> list[Trial[CandidateT]]
```

Await the completion of multiple trials.

**Parameters:**

* **`*trials`**
  (`Trial[CandidateT]`, default:
  `()`
  )
  –The trials to wait for.

**Returns:**

* `list[Trial[CandidateT]]`
  –A future that resolves to a list of completed trials.

<Accordion title="Source code in dreadnode/optimization/trial.py" icon="code">
```python
@staticmethod
async def wait_for(*trials: "Trial[CandidateT]") -> "list[Trial[CandidateT]]":
    """
    Await the completion of multiple trials.

    Args:
        *trials: The trials to wait for.

    Returns:
        A future that resolves to a list of completed trials.
    """
    return await asyncio.gather(*(trial._future for trial in trials))  # noqa: SLF001
```


</Accordion>

TrialCollector
--------------

Collect a list of relevant trials based on the current trial.

TrialSampler
------------

Sample from a list of trials.

Distribution
------------

```python
Distribution()
```

Base class for all search space distributions.

beam\_search
------------

```python
beam_search(
    transform: TransformLike[
        list[Trial[CandidateT]], CandidateT
    ],
    initial_candidate: CandidateT,
    *,
    beam_width: int = 3,
    branching_factor: int = 3,
    context_depth: int = 5,
) -> Search[CandidateT]
```

Creates a graph search configured for classic beam search.

This strategy maintains parallel reasoning paths by keeping a "beam" of the top `k`
best trials from the previous step. Each trial in the beam is expanded independently,
using its own lineage for context.

**Parameters:**

* **`transform`**
  (`TransformLike[list[Trial[CandidateT]], CandidateT]`)
  –The function that takes the history and generates new candidates.
* **`initial_candidate`**
  (`CandidateT`)
  –The starting point for the refinement chain.
* **`beam_width`**
  (`int`, default:
  `3`
  )
  –The number of top candidates to keep at each step (the 'k').
* **`branching_factor`**
  (`int`, default:
  `3`
  )
  –How many new candidates to generate from each trial in the beam.

**Returns:**

* `Search[CandidateT]`
  –A pre-configured GraphSearch instance.

<Accordion title="Source code in dreadnode/optimization/search/graph.py" icon="code">
```python
def beam_search(
    transform: TransformLike[list[Trial[CandidateT]], CandidateT],
    initial_candidate: CandidateT,
    *,
    beam_width: int = 3,
    branching_factor: int = 3,
    context_depth: int = 5,
) -> Search[CandidateT]:
    """
    Creates a graph search configured for classic beam search.

    This strategy maintains parallel reasoning paths by keeping a "beam" of the top `k`
    best trials from the previous step. Each trial in the beam is expanded independently,
    using its own lineage for context.

    Args:
        transform: The function that takes the history and generates new candidates.
        initial_candidate: The starting point for the refinement chain.
        beam_width: The number of top candidates to keep at each step (the 'k').
        branching_factor: How many new candidates to generate from each trial in the beam.

    Returns:
        A pre-configured GraphSearch instance.
    """
    return graph_search(
        transform=transform,
        initial_candidate=initial_candidate,
        branching_factor=branching_factor,
        context_collector=lineage.configure(depth=context_depth),
        pruning_sampler=top_k.configure(k=beam_width),
        name="beam_search",
    )
```


</Accordion>

binary\_image\_search
---------------------

```python
binary_image_search(
    start_image: Image,
    end_image: Image,
    *,
    tolerance: float = 0.01,
    distance_method: DistanceMethod = "l2",
    decision_objective: str | None = None,
    decision_threshold: float = 0.0,
) -> Search[Image]
```

Performs a binary search between two images to find a new image
which lies on the decision boundary defined by the objective and threshold.

**Parameters:**

* **`start_image`**
  (`Image`)
  –An image expected to be unsuccessful (score <= [decision\_threshold]).
* **`end_image`**
  (`Image`)
  –An image expected to be successful (score > [decision\_threshold]).
* **`tolerance`**
  (`float`, default:
  `0.01`
  )
  –The maximum acceptable distance between the start and end images.
* **`distance_method`**
  (`DistanceMethod`, default:
  `'l2'`
  )
  –The distance metric to use for measuring similarity.
* **`decision_objective`**
  (`str | None`, default:
  `None`
  )
  –The name of the objective to use for the decision. If None,

<Accordion title="Source code in dreadnode/optimization/search/boundary.py" icon="code">
```python
def binary_image_search(
    start_image: Image,
    end_image: Image,
    *,
    tolerance: float = 1e-2,  # We assume normalized distance
    distance_method: DistanceMethod = "l2",
    decision_objective: str | None = None,
    decision_threshold: float = 0.0,
) -> Search[Image]:
    """
    Performs a binary search between two images to find a new image
    which lies on the decision boundary defined by the objective and threshold.

    Args:
        start_image: An image expected to be unsuccessful (score <= [decision_threshold]).
        end_image: An image expected to be successful (score > [decision_threshold]).
        tolerance: The maximum acceptable distance between the start and end images.
        distance_method: The distance metric to use for measuring similarity.
        decision_objective: The name of the objective to use for the decision. If None,
    """
    from dreadnode.transforms.image import interpolate

    async def tolerable(img1: Image, img2: Image) -> bool:
        metric = await image_distance(img1, method=distance_method)(img2)
        return metric.value < tolerance

    return boundary_search(
        start_candidate=start_image,
        end_candidate=end_image,
        interpolate=interpolate(alpha=0.5),
        tolerable=tolerable,
        decision_objective=decision_objective,
        decision_threshold=decision_threshold,
    )
```


</Accordion>

boundary\_search
----------------

```python
boundary_search(
    start_candidate: CandidateT,
    end_candidate: CandidateT,
    interpolate: TransformLike[
        tuple[CandidateT, CandidateT], CandidateT
    ],
    tolerable: Callable[
        [CandidateT, CandidateT], Awaitable[bool]
    ],
    *,
    decision_objective: str | None = None,
    decision_threshold: float = 0.0,
) -> Search[CandidateT]
```

Performs a boundary search between two candidates to find a new candidate
which lies on the decision boundary defined by the objective and threshold.

**Parameters:**

* **`start_candidate`**
  (`CandidateT`)
  –A candidate expected to be unsuccessful (score <= [decision\_threshold]).
* **`end_candidate`**
  (`CandidateT`)
  –A candidate expected to be successful (score > [decision\_threshold]).
* **`interpolate`**
  (`TransformLike[tuple[CandidateT, CandidateT], CandidateT]`)
  –A transform that takes two candidates and returns a candidate
  that is between them.
* **`tolerable`**
  (`Callable[[CandidateT, CandidateT], Awaitable[bool]]`)
  –A function that checks if the similarity (distance) between two candidates is within acceptable limits.
* **`decision_objective`**
  (`str | None`, default:
  `None`
  )
  –The name of the objective to use for the decision. If None, uses the overall trial score.
* **`decision_threshold`**
  (`float`, default:
  `0.0`
  )
  –The threshold value for the decision objective.

<Accordion title="Source code in dreadnode/optimization/search/boundary.py" icon="code">
```python
def boundary_search(
    start_candidate: CandidateT,
    end_candidate: CandidateT,
    interpolate: TransformLike[tuple[CandidateT, CandidateT], CandidateT],
    tolerable: t.Callable[[CandidateT, CandidateT], t.Awaitable[bool]],
    *,
    decision_objective: str | None = None,
    decision_threshold: float = 0.0,
) -> Search[CandidateT]:
    """
    Performs a boundary search between two candidates to find a new candidate
    which lies on the decision boundary defined by the objective and threshold.

    Args:
        start_candidate: A candidate expected to be unsuccessful (score <= [decision_threshold]).
        end_candidate: A candidate expected to be successful (score > [decision_threshold]).
        interpolate: A transform that takes two candidates and returns a candidate
                     that is between them.
        tolerable: A function that checks if the similarity (distance) between two candidates is within acceptable limits.
        decision_objective: The name of the objective to use for the decision. If None, uses the overall trial score.
        decision_threshold: The threshold value for the decision objective.
    """

    async def search(context: OptimizationContext) -> t.AsyncGenerator[Trial[CandidateT], None]:
        if decision_objective and decision_objective not in context.objective_names:
            raise ValueError(
                f"Decision objective '{decision_objective}' not found in the optimization context."
            )

        def is_successful(trial: Trial) -> bool:
            return trial.get_score(decision_objective) > decision_threshold

        start_trial = Trial(candidate=start_candidate)
        end_trial = Trial(candidate=end_candidate)
        yield start_trial
        yield end_trial

        await Trial.wait_for(start_trial, end_trial)

        if is_successful(start_trial):
            raise ValueError(
                f"start_candidate was considered successful ({decision_objective or 'score'} > {decision_threshold}): {start_trial.scores}."
            )

        if not is_successful(end_trial):
            raise ValueError(
                f"end_candidate was not considered successful ({decision_objective or 'score'} <= {decision_threshold}): {end_trial.scores}."
            )

        original_bound = start_candidate
        adversarial_bound = end_candidate
        interpolate_transform = Transform(interpolate)

        # TODO(nick): When tolerance is met immediately, it can be confusing that
        # the attack just returns as search_exhausted. Maybe we add some kind of log
        # reason to be put in the Trial?
        while not await tolerable(original_bound, adversarial_bound):
            midpoint_candidate = await interpolate_transform((original_bound, adversarial_bound))
            if inspect.isawaitable(midpoint_candidate):
                midpoint_candidate = await midpoint_candidate

            midpoint_trial = Trial(candidate=midpoint_candidate)
            yield midpoint_trial
            await midpoint_trial

            if is_successful(midpoint_trial):
                adversarial_bound = midpoint_trial.candidate
            else:
                original_bound = midpoint_trial.candidate

        yield Trial(candidate=adversarial_bound)

    return Search(search, name="boundary_search")
```


</Accordion>

graph\_neighborhood\_search
---------------------------

```python
graph_neighborhood_search(
    transform: TransformLike[
        list[Trial[CandidateT]], CandidateT
    ],
    initial_candidate: CandidateT,
    *,
    neighborhood_depth: int = 2,
    frontier_size: int = 5,
    branching_factor: int = 3,
) -> Search[CandidateT]
```

Creates a graph search configured with a local neighborhood context, where the trial context
passed to the transform includes the trials in the local neighborhood up to `2h-1` distance
away where `h` is the neighborhood depth. This means the trials which are "parents",
"grandparents", "uncles", or "cousins" can be considered during the creation of new nodes.

Once the pool of candidate trials is established, `frontier_size` determines how many of
the best candidates are kept for the iteration.

See: "Graph of Attacks" - https://arxiv.org/pdf/2504.19019v1

**Parameters:**

* **`transform`**
  (`TransformLike[list[Trial[CandidateT]], CandidateT]`)
  –The function that takes the neighborhood context and generates new candidates.
* **`initial_candidate`**
  (`CandidateT`)
  –The starting point for the search.
* **`neighborhood_depth`**
  (`int`, default:
  `2`
  )
  –The depth 'h' used to calculate the size of the local neighborhood context.
* **`frontier_size`**
  (`int`, default:
  `5`
  )
  –The number of top candidates to form the next generation's frontier ('d').
* **`branching_factor`**
  (`int`, default:
  `3`
  )
  –How many new candidates to generate from each current leaf node.

**Returns:**

* `Search[CandidateT]`
  –A pre-configured GraphSearch instance.

<Accordion title="Source code in dreadnode/optimization/search/graph.py" icon="code">
```python
def graph_neighborhood_search(
    transform: TransformLike[list[Trial[CandidateT]], CandidateT],
    initial_candidate: CandidateT,
    *,
    neighborhood_depth: int = 2,
    frontier_size: int = 5,
    branching_factor: int = 3,
) -> Search[CandidateT]:
    """
    Creates a graph search configured with a local neighborhood context, where the trial context
    passed to the transform includes the trials in the local neighborhood up to `2h-1` distance
    away where `h` is the neighborhood depth. This means the trials which are "parents",
    "grandparents", "uncles", or "cousins" can be considered during the creation of new nodes.

    Once the pool of candidate trials is established, `frontier_size` determines how many of
    the best candidates are kept for the iteration.

    See: "Graph of Attacks" - https://arxiv.org/pdf/2504.19019v1

    Args:
        transform: The function that takes the neighborhood context and generates new candidates.
        initial_candidate: The starting point for the search.
        neighborhood_depth: The depth 'h' used to calculate the size of the local neighborhood context.
        frontier_size: The number of top candidates to form the next generation's frontier ('d').
        branching_factor: How many new candidates to generate from each current leaf node.

    Returns:
        A pre-configured GraphSearch instance.
    """
    return graph_search(
        transform=transform,
        initial_candidate=initial_candidate,
        branching_factor=branching_factor,
        context_collector=local_neighborhood.configure(depth=neighborhood_depth),
        pruning_sampler=top_k.configure(k=frontier_size),
        name="graph_neighborhood_search",
    )
```


</Accordion>

graph\_search
-------------

```python
graph_search(
    transform: TransformLike[
        list[Trial[CandidateT]], CandidateT
    ],
    initial_candidate: CandidateT,
    *,
    branching_factor: int = 3,
    context_collector: TrialCollector[CandidateT] = lineage,
    pruning_sampler: TrialSampler[CandidateT] = top_k,
    name: str = "graph_search",
) -> Search[CandidateT]
```

Creates a generalized, stateful strategy for generative graph-based search.

Formally, the structure is a connected directed acyclic graph (DAG) where nodes represent
trials and edges are parent-child relationships.

For each iteration, it:
1 - Gathers related trials using `context_collector` for every leaf node
2 - Applies the `transform` to [leaf, \*context] `branching_factor` times for each leaf
3 - Suggests all new children for evaluation
4 - Waits for all children to complete
5 - Prunes with `pruning_sampler` to establish leaves for the next step

<Accordion title="Source code in dreadnode/optimization/search/graph.py" icon="code">
```python
def graph_search(
    transform: TransformLike[list[Trial[CandidateT]], CandidateT],
    initial_candidate: CandidateT,
    *,
    branching_factor: int = 3,
    context_collector: TrialCollector[CandidateT] = lineage,
    pruning_sampler: TrialSampler[CandidateT] = top_k,
    name: str = "graph_search",
) -> Search[CandidateT]:
    """
    Creates a generalized, stateful strategy for generative graph-based search.

    Formally, the structure is a connected directed acyclic graph (DAG) where nodes represent
    trials and edges are parent-child relationships.

    For each iteration, it:
        1 - Gathers related trials using `context_collector` for every leaf node
        2 - Applies the `transform` to [leaf, *context] `branching_factor` times for each leaf
        3 - Suggests all new children for evaluation
        4 - Waits for all children to complete
        5 - Prunes with `pruning_sampler` to establish leaves for the next step
    """

    async def search(
        _: OptimizationContext,
        *,
        transform: TransformLike[list[Trial[CandidateT]], CandidateT] = Config(transform),  # noqa: B008
        initial_candidate: CandidateT = Config(initial_candidate),  # noqa: B008
        branching_factor: int = Config(branching_factor),
        context_collector: TrialCollector[CandidateT] = Config(context_collector),  # noqa: B008
        pruning_sampler: TrialSampler[CandidateT] = Config(pruning_sampler),  # noqa: B008
    ) -> t.AsyncGenerator[Trial[CandidateT], None]:
        trials: list[Trial[CandidateT]] = []
        leaves: list[Trial[CandidateT]] = []
        transform = Transform.fit(transform)

        initial_trial = Trial(candidate=initial_candidate)
        yield initial_trial
        await initial_trial

        if initial_trial.status != "finished":
            return

        trials.append(initial_trial)
        leaves = [initial_trial]

        while leaves:
            # Generate all new trials branching from current leaves
            new_trials: list[Trial[CandidateT]] = []
            for leaf in leaves:
                trials_context = [leaf, *context_collector(leaf, trials)]
                coroutines = [transform(trials_context) for _ in range(branching_factor)]
                async with concurrent_gen(coroutines) as gen:
                    async for candidate in gen:
                        new_trial = Trial(candidate=candidate, parent_id=leaf.id)
                        new_trials.append(new_trial)
                        yield new_trial

            # Wait for all new trials to complete
            await Trial.wait_for(*new_trials)

            # Collect finished trials and prune to get new leaves
            finished = [t for t in new_trials if t.status == "finished"]
            trials.extend(finished)
            interleaved = interleave_by_parent(finished)
            leaves = pruning_sampler(interleaved)

    return Search(search, name=name)
```


</Accordion>

iterative\_search
-----------------

```python
iterative_search(
    transform: TransformLike[
        list[Trial[CandidateT]], CandidateT
    ],
    initial_candidate: CandidateT,
    *,
    branching_factor: int = 1,
) -> Search[CandidateT]
```

Creates a GraphSearch configured for single-path iterative refinement.

This strategy maintains a single path of reasoning by always expanding from the
single best trial of the previous step. The context for refinement is the
direct lineage of that best trial.

Set `branching_factor` > 1 to explore multiple candidates at each step.

**Parameters:**

* **`transform`**
  (`TransformLike[list[Trial[CandidateT]], CandidateT]`)
  –The function that takes the history and generates new candidates.
* **`initial_candidate`**
  (`CandidateT`)
  –The starting point for the refinement chain.
* **`branching_factor`**
  (`int`, default:
  `1`
  )
  –How many new candidates to generate from the best trial at each step.
  The best of these will be chosen for the next step.

**Returns:**

* `Search[CandidateT]`
  –A pre-configured graph search instance.

<Accordion title="Source code in dreadnode/optimization/search/graph.py" icon="code">
```python
def iterative_search(
    transform: TransformLike[list[Trial[CandidateT]], CandidateT],
    initial_candidate: CandidateT,
    *,
    branching_factor: int = 1,
) -> Search[CandidateT]:
    """
    Creates a GraphSearch configured for single-path iterative refinement.

    This strategy maintains a single path of reasoning by always expanding from the
    single best trial of the previous step. The context for refinement is the
    direct lineage of that best trial.

    Set `branching_factor` > 1 to explore multiple candidates at each step.

    Args:
        transform: The function that takes the history and generates new candidates.
        initial_candidate: The starting point for the refinement chain.
        branching_factor: How many new candidates to generate from the best trial at each step.
                          The best of these will be chosen for the next step.

    Returns:
        A pre-configured graph search instance.
    """
    return graph_search(
        transform=transform,
        initial_candidate=initial_candidate,
        branching_factor=branching_factor,
        context_collector=lineage,
        pruning_sampler=top_k.configure(k=1),
        name="iterative_search",
    )
```


</Accordion>

optuna\_search
--------------

```python
optuna_search(
    search_space: SearchSpace,
    *,
    sampler: BaseSampler | None = None,
) -> Search[AnyDict]
```

Creates a search strategy that uses Optuna for Bayesian optimization.

This strategy leverages Optuna's powerful samplers (like TPE) to intelligently
explore a defined search space, learning from past trial results to suggest
more promising candidates.

**Parameters:**

* **`search_space`**
  (`SearchSpace`)
  –The search space to explore, defining parameter names and distributions.
* **`sampler`**
  (`BaseSampler | None`, default:
  `None`
  )
  –An optional Optuna sampler (e.g., TPESampler, NSGAIISampler).

<Accordion title="Source code in dreadnode/optimization/search/optuna_.py" icon="code">
```python
def optuna_search(
    search_space: SearchSpace,
    *,
    sampler: optuna.samplers.BaseSampler | None = None,
) -> Search[AnyDict]:
    """
    Creates a search strategy that uses Optuna for Bayesian optimization.

    This strategy leverages Optuna's powerful samplers (like TPE) to intelligently
    explore a defined search space, learning from past trial results to suggest
    more promising candidates.

    Args:
        search_space: The search space to explore, defining parameter names and distributions.
        sampler: An optional Optuna sampler (e.g., TPESampler, NSGAIISampler).
    """

    async def search(
        context: OptimizationContext,
        *,
        search_space: SearchSpace = search_space,
        sampler: optuna.samplers.BaseSampler | None = sampler,
    ) -> t.AsyncGenerator[Trial[AnyDict], None]:
        optuna_study = optuna.create_study(directions=context.directions, sampler=sampler)
        optuna_search_space = _convert_search_space(search_space)
        objective_names = context.objective_names

        while True:
            optuna_trial = optuna_study.ask(optuna_search_space)

            trial = Trial[AnyDict](candidate=optuna_trial.params)
            yield trial
            await trial

            if trial.status == "finished":
                # Provide scores in the correct order for multi-objective optimization.
                scores = [trial.get_score(name, default=0.0) for name in objective_names]
                optuna_study.tell(optuna_trial, scores)
            else:
                state = (
                    optuna.trial.TrialState.PRUNED
                    if trial.status == "pruned"
                    else optuna.trial.TrialState.FAIL
                )
                optuna_study.tell(optuna_trial, state=state)

    return Search(search)
```


</Accordion>

random\_search
--------------

```python
random_search(
    search_space: SearchSpace, *, seed: float | None = None
) -> Search[AnyDict]
```

Create a search strategy that suggests candidates by sampling uniformly and
independently from the search space at each step.

This strategy is "memoryless" and does not learn from the results of
past trials. It is primarily useful as a simple baseline for comparing
the performance of more sophisticated optimization algorithms.

**Parameters:**

* **`search_space`**
  (`SearchSpace`)
  –The search space to explore.
* **`seed`**
  (`float | None`, default:
  `None`
  )
  –The random seed to use for reproducibility.

<Accordion title="Source code in dreadnode/optimization/search/random.py" icon="code">
```python
def random_search(search_space: SearchSpace, *, seed: float | None = None) -> Search[AnyDict]:
    """
    Create a search strategy that suggests candidates by sampling uniformly and
    independently from the search space at each step.

    This strategy is "memoryless" and does not learn from the results of
    past trials. It is primarily useful as a simple baseline for comparing
    the performance of more sophisticated optimization algorithms.

    Args:
        search_space: The search space to explore.
        seed: The random seed to use for reproducibility.
    """

    async def search(
        _: OptimizationContext, *, seed: float | None = seed
    ) -> t.AsyncGenerator[Trial[AnyDict], None]:
        _random = random.Random(seed)  # noqa: S311 # nosec
        while True:
            yield Trial(candidate=_sample_from_space(search_space, _random))

    return Search(search, name="random_search")
```


</Accordion>