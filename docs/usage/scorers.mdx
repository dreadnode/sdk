---
title: 'Scorers'
description: 'Learn how to measure and evaluate task outputs automatically using built-in and custom scorers.'
public: true
---

Scorers are the heart of automated evaluation in Strikes. They provide a powerful, reusable way to measure the quality, safety, and correctness of your task outputs without cluttering your core logic.

## What is a Scorer?

A **Scorer** is a function that evaluates data—typically the output of a task—and automatically logs a `dreadnode.Metric` result. By decoupling evaluation logic from task execution, you can build cleaner, more modular, and highly reusable testing and red-teaming harnesses.

Consider a task that generates a JSON response.

**Before Scorers**, you might write evaluation logic directly inside your task:

```python
import json

@dn.task()
async def generate_json(prompt: str) -> str:
    # Assume this function calls an LLM to generate a JSON string
    output = call_my_llm(prompt)

    # Manual evaluation logic inside the task
    try:
        json.loads(output)
        dn.log_metric("is_valid_json", 1.0)
    except json.JSONDecodeError:
        dn.log_metric("is_valid_json", 0.0)

    return output
```

**With Scorers**, the evaluation logic is cleanly separated:

```python
from dreadnode import scorers

@dn.task(scorers=[scorers.is_json()])
async def generate_json(prompt: str) -> str:
    # The task now only focuses on its primary job
    return call_my_llm(prompt)
```

The result is the same—a metric named `is_json` is logged—but the code is far more maintainable and the scorer is now reusable across any task that's supposed to produce JSON.

When the task runs, the scorer will automatically:
1. Receive the task's output
2. Evaluate it according to your logic
3. Log a metric with the scoring function's name and returned value
4. Associate the metric with the task's output object

## Using Built-in Scorers

Strikes includes a rich library of pre-built scorers in `dreadnode.scorers` to cover common evaluation needs.

<Note>
Some scorers rely on extra Python packages (e.g., `textstat`, `nltk`, `presidio-analyzer`). If a required package is not installed, the scorer will be disabled and Strikes will log a `UserWarning` in your console with installation instructions.
</Note>

### Content & Safety

These scorers inspect text for specific types of content.

- **`contains()`**: Checks for a substring or regex pattern.
- **`detect_refusal()`**: Detects common LLM refusal phrases ("I can't assist with that").
- **`detect_bias()`**: Looks for stereotypical or biased language.
- **`detect_sensitive_keywords()`**: Looks for words like "password" or "api_key".
- **`detect_unsafe_shell_content()`**: Finds potentially dangerous shell commands.
- **`detect_ansi_escapes()`**: Detects unrendered terminal color codes.
- **`detect_pii()`**: Uses regex (`detect_pii`) or the powerful Presidio library (`detect_pii_with_presidio`) to find PII.
- **`detect_harm_with_openai()`**: Uses the OpenAI Moderation API to score harmfulness.

```python
# Example: Ensure the model output is not a refusal
@dn.task(scorers=[
    scorers.detect_refusal(name="is_refusal")
])
async def generate_code(request: str) -> str:
    # ... generation logic ...
```

### Readability & Sentiment

These scorers analyze the linguistic style and emotional tone of the text.

- **`readability()`**: Scores readability against a target grade level using the Flesch-Kincaid formula.
- **`sentiment()`**: Scores sentiment (positive, negative, neutral) using TextBlob.
- **`sentiment_with_perspective()`**: Scores attributes like "TOXICITY" or "INSULT" using Google's Perspective API.

```python
# Example: Check if a public-facing message has positive sentiment and is easy to read.
@dn.task(scorers=[
    scorers.sentiment(target="positive"),
    scorers.readability(target_grade=8.0)
])
async def generate_customer_reply(ticket: dict) -> str:
    # ... logic to generate a reply ...
```

### Format Validation

These scorers check if the output conforms to a specific data format.

- **`is_json()`**: Validates if the output is a well-formed JSON string.
- **`is_xml()`**: Validates if the output is a well-formed XML string.

```python
# Example: Validate the output of a function calling agent
@dn.task(scorers=[
    scorers.is_json(name="is_valid_tool_call")
])
async def get_tool_call(prompt: str) -> str:
    # ... logic to generate a JSON tool call ...
```

### Similarity & Consistency

These scorers compare the output to a reference text.

- **`similarity()`**: Fast comparison using Python's `difflib`.
- **`similarity_with_tf_idf()`**: Uses TF-IDF vectors for semantic similarity.
- **`similarity_with_sentence_transformers()`**: Uses embedding cosine similarity with Sentence Transformers.
- **`similarity_with_litellm()`**: Uses any litellm-supported embedding model for cosine similarity.
- **`bleu()`**: Measures n-gram overlap, common in machine translation evaluation.
- **`character_consistency()`**: Compares the distribution of letters, numbers, and symbols.

```python
# Example: Check if the generated summary is semantically similar to a reference summary
reference_summary = "The protagonist discovers a hidden artifact and embarks on a quest."

@dn.task(scorers=[
    scorers.similarity_with_sentence_transformers(reference=reference_summary)
])
async def summarize_story(story_text: str) -> str:
    # ... summarization logic ...
```

### Length & Lexical Diversity

These scorers analyze the structural properties of the text.

- **`length_in_range()`**: Checks if character length is within a min/max bound.
- **`length_ratio()`**: Compares output length to a reference text's length.
- **`length_target()`**: Checks if output length is close to a target value.
- **`type_token_ratio()`**: Measures lexical diversity (unique words / total words).

```python
# Example: Ensure the output is between 100 and 500 characters
@dn.task(scorers=[
    scorers.length_in_range(min_length=100, max_length=500)
])
async def generate_short_bio(person_info: dict) -> str:
    # ... bio generation logic ...
```

### LLM-based Evaluation

Sometimes, the best way to evaluate an LLM's output is with another, more powerful LLM.

- **`llm_judge()`**: Uses a "judge" model to grade an output against a natural language rubric.

```python
# Example: Use GPT-4o to check if the output is funny

@dn.task(scorers=[
    scorers.llm_judge(
        model="gpt-4o",
        rubric="The output should be a witty, single-sentence pun related to technology.",
        name="is_funny"
    )
])
async def tell_a_joke(topic: str) -> str:
    # ... logic to generate a joke from a weaker model ...
```

### Scoring Chat-based Outputs

If your task output is a `rigging.Chat` object, you can use `scorers.wrap_chat` to apply any text-based scorer to its messages. This adapter lets you filter which messages to evaluate.

```python
from dreadnode.scorers.rigging import wrap_chat

# Create a scorer that only checks the last assistant message for refusals
check_last_response = wrap_chat(
    scorers.detect_refusal(),
    filter="last_assistant"
)

@dn.task(scorers=[check_last_response])
async def chat_with_agent(prompt: str) -> "Chat":
    # ... chat logic that returns a Chat object ...
```

## Calling Scorers Directly

While attaching scorers to a `@dn.task` decorator is the most common pattern, it's not the only way to use them. **Scorers are simply callable objects.** You can invoke them at any point in your code to perform an evaluation and then log the result manually.

This pattern is incredibly useful for more complex scenarios where evaluation isn't just a simple check on the final output of a task.

The process is straightforward:
1.  Instantiate a scorer using its factory function (e.g., `my_scorer = scorers.is_json()`).
2.  `await` the scorer with the data you want to evaluate. It will return a `dreadnode.Metric` object.
3.  Pass this `Metric` object directly to `dreadnode.log_metric()`.

```python
from dreadnode import scorers

@dn.task()
async def process_data_conditionally(data: str):
    # We can call a scorer manually inside the task
    json_scorer = scorers.is_json()
    metric = await json_scorer(data)
    dn.log_metric("valid_json", metric) # or dn.log_metric(json_scorer.name, metric)

    if metric.value == 1.0:
        # Perform more expensive processing only if the data is valid JSON
        # ...
```

This pattern unlocks some more advanced evaluation strategies:

**1. Conditional Evaluation**

Run an expensive scorer (like `llm_judge`) only if a cheaper, preliminary check passes.

```python
@dn.task()
async def evaluate_response(response: str):
    # First, run a cheap check
    refusal_scorer = scorers.detect_refusal()
    refusal_metric = await refusal_scorer(response)
    dn.log_metric(refusal_metric)

    # Only run the expensive judge if the first check passed (value is 0.0)
    if refusal_metric.value == 0.0:
        judge_scorer = scorers.llm_judge(model="gpt-4", rubric="Is the response helpful?")
        judgement_metric = await judge_scorer(response)
        dn.log_metric(judgement_metric)

```

**2. Evaluating Intermediate Steps**

Score multiple, intermediate results generated within a single task.

```python
@dn.task()
async def multi_step_generation(prompt: str) -> str:
    # Step 1: Generate an outline
    outline = await generate_outline(prompt)
    dn.log_metric(
        "outline_length",
        await scorers.length_in_range(min_length=50)(outline)
    )

    # Step 2: Generate the full text from the outline
    full_text = await generate_from_outline(outline)
    dn.log_metric(
        "readability_score",
        await scorers.readability(target_grade=8)(full_text)
    )

    return full_text
```

Although, the pattern above might be better represented as individual tasks:

```python
@dn.task(scorers=[scorers.length_in_range(min_length=50)])
async def generate_outline(prompt: str) -> str:
    # ... outline generation logic ...

@dn.task(scorers=[scorers.readability(target_grade=8)])
async def generate_from_outline(outline: str) -> str:
    # ... full text generation logic ...

@dn.task()
async def multi_step_generation(prompt: str) -> str:
    outline = await generate_outline(prompt)
    full_text = await generate_from_outline(outline)
    return full_text
```

## Dynamic Values with Lookups

What if a scorer needs to compare the task's output to its *input*? Or to a run-level parameter? Hard-coding these values isn't practical.

The solution is `Lookup`, a declarative "pointer" to data that is resolved automatically at runtime. You can create them with these helpers:
- `dreadnode.lookup_input(name)`
- `dreadnode.lookup_output(name)`
- `dreadnode.lookup_param(name)`

This makes your scorers highly reusable, as they can adapt to the context of any task they're attached to.

```python
# Example: Score similarity against the task's own input prompt
@dn.task(scorers=[
    scorers.similarity(reference=lookup_input("prompt"))
])
async def rephrase_text(prompt: str) -> str:
    # ... rephrasing logic ...

# Example: Ensure length is close to a target defined in the run's parameters
with dn.run("length-experiment"):
    dn.log_param("target_length", 150)

    @dn.task(scorers=[
        scorers.length_target(target_length=lookup_param("target_length"))
    ])
    async def generate_text():
        # ...
```

<Tip>
Using Lookups is a best practice. It allows you to define a generic evaluation harness (a task with scorers) and apply it to different inputs and configurations without changing the scorer code.
</Tip>

## Writing Your Own Scorers

A scorer can be any function (sync or async) that accepts the data to be scored.

### Simple Scorers

The simplest scorer is a function that takes the task's output as an argument and returns a `float` between 0.0 and 1.0.

```python
# A custom scorer to check for politeness
async def is_polite(output: str) -> float:
    if "please" in output.lower() or "thank you" in output.lower():
        return 1.0
    return 0.0

@dn.task(scorers=[is_polite])
async def make_request(request: str) -> str:
    # ...
```

### Scorers with Rich Metadata

For more detailed feedback, your scorer can return a `dreadnode.Metric` object. This allows you to include a `value` as well as structured `attributes`.

```python
from dreadnode import Metric

# A scorer that returns a reason for its score
async def has_codeblock(output: str) -> Metric:
    if "```" in output:
        return Metric(value=1.0, attributes={"reason": "Code block found."})
    else:
        return Metric(value=0.0, attributes={"reason": "No code block detected."})

@dn.task(scorers=[has_codeblock])
async def write_python_code(prompt: str) -> str:
    # ...
```

<Tip>
Returning a full `Metric` object is highly recommended for custom scorers. The `attributes` provide crucial context that helps you understand *why* a score was given, which is essential for debugging and improving your system.
</Tip>

### The Factory Pattern

Frequently your scorer will need some configuration data to properly work. However, when you attach a scorer to `@dn.task`, Strikes automatically passes the task's output to the **first argument** of your scorer function. Any other parameters must be provided when you instantiate the scorer.

In many libraries this is done using class primitives, but in general we prefer closures (functions that return other functions with enclosed state) as they feel more functional and quick to parse.

The best practice, used by all of Strikes' built-in scorers, is to create a **factory function** that configures and returns your scorer.

<Tip>
A "scorer factory" is a function that takes configuration (like a reference text), and returns the actual scorer function (the one that will perform the evaluation). This allows you to create pre-configured, reusable scorer instances.
</Tip>

```python
import typing as t
from difflib import SequenceMatcher
from dreadnode import Scorer

def custom_similarity(reference: str) -> Scorer[t.Any]:
    """A factory that creates our configurable similarity scorer."""

    # The actual evaluation logic is inside this inner function.
    # It will receive the task's output as its only argument.
    async def evaluate(data_to_score: t.Any) -> float:
        return SequenceMatcher(a=reference, b=str(data_to_score)).ratio()

    # 3. Return a Scorer instance from the `evaluate` callable.
    return Scorer.from_callable(evaluate)

@dn.task(scorers=[
    custom_similarity(reference="hello world"),
])
async def my_task(prompt: str) -> str:
    return "hello universe" # This will be the `data_to_score`
```

### Handling Lookups in Scorers

Your custom scorers can be made even more powerful and reusable by supporting `Lookup` objects for their parameters. This let's users supply lazy references to data that will be resolved at runtime (from the current run or task). There is some small overhead to handle as we need to resolve the `Lookup` before using it in our scoring logic, but it's well worth it for the flexibility it provides.

```python
from dreadnode import Scorer

async def naive_starts_with(reference: str) -> Scorer[str]:
    def _naive_starts_with(output: str) -> float:
        # Assume `reference` is a string, but it might be a Lookup
        # This will raise an error if `reference` is a Lookup
        if output.startswith(reference):
            return 1.0
        return 0.0

    return Scorer.from_callable(evaluate)
```

The solution is the `dreadnode.resolve_lookup()` utility function. It safely resolves a value that might be a `Lookup`:

- If the value is a `Lookup`, `resolve_lookup()` fetches the actual value from the run/task context.
- If the value is already a direct value (like a string or number), `resolve_lookup()` simply returns it unchanged.

By wrapping your scorer's parameters with `resolve_lookup()`, you can transparently handle both static values and dynamic `Lookup`s.

```python
from dreadnode import resolve_lookup, Lookup

async def starts_with(reference: str | Lookup) -> Scorer[str]:
    async def _starts_with(output: str) -> float:
        # Resolve the reference, which might be a Lookup or a static value
        resolved_reference = str(resolve_lookup(reference))

        if output.startswith(resolved_reference):
            return 1.0
        return 0.0

    return Scorer.from_callable(_starts_with)

@dn.task(scorers=[
    # Use with a dynamic Lookup
    starts_with(reference=lookup_input("prompt")),
])
async def my_task(prompt: str) -> str:
    # Let's say prompt is "hello there"
    return "hello universe" # This will be the `data_to_score`
```

Generally, it's good practice to consider any scoring constructor arguments that might benefit from being `Lookup`s. This allows users to pass dynamic values when attaching the scorer to a task.

## Composing Behavior with Operators

Operators are higher-order functions that wrap or modify existing scorers. They let you adapt and reuse scorers without rewriting them.

### `invert()`

Flips a score (`value` becomes `1.0 - value`). This is useful for turning a "detection" scorer into a "passes if not detected" scorer.

```python
# Create a scorer that passes if NO sensitive keywords are found
no_sensitive_keywords = scorers.invert(
    scorers.detect_sensitive_keywords(),
    name="no_sensitive_keywords"
)

@dn.task(scorers=[no_sensitive_keywords])
async def process_user_data(data: dict) -> str:
    # ...
```

### `threshold()`

Converts a continuous score (e.g., 0.0 to 1.0) into a binary pass/fail result.

```python
# Create a scorer that passes only if similarity is above 80%
is_similar_enough = scorers.threshold(
    scorers.similarity(reference="expected answer"),
    gt=0.8, # greater than 0.8
    name="is_similar_enough"
)

@dn.task(scorers=[is_similar_enough])
async def answer_question(question: str) -> str:
    # ...
```

### `scale()`

Remaps a score from its original range to a new one (e.g., 0-1 to 1-5 for a star rating).

```python
# Create a scorer that rates readability on a 1-5 scale
# The readability scorer's value is 0-1, so we scale it.
readability_rating = scorers.scale(
    scorers.readability(target_grade=8),
    new_min=1,
    new_max=5,
    name="readability_rating"
)

@dn.task(scorers=[readability_rating])
async def write_public_announcement(details: dict) -> str:
    # ...
```

<Tip>
When using operators, always provide a new `name`. This ensures the resulting metric is clearly identified in the UI and avoids confusion with the original, unwrapped scorer's metric.
</Tip>
```