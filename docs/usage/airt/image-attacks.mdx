---
title: "Adversarial Image Attacks"
description: "Generate imperceptible perturbations in images to fool classifiers and other computer vision models."
public: true
---

Adversarial image attacks aim to modify a source image with minimal, often human-imperceptible, changes to cause a model to misclassify it or behave unexpectedly. The AIRT framework provides search strategies designed specifically for this task.

## Running an Image Misclassification Attack

Let's configure an attack that attempts to make a model misclassify an image of a cat as a "Granny Smith" apple. This requires first defining a `CustomTarget` that wraps our image classification model.

You can run this full example directly.

```python
import dreadnode as dn
from dreadnode.airt import Attack, simba_search
from dreadnode.airt.target import CustomTarget
from dreadnode.scorers import json_path, image_distance

# Ensure dreadnode is configured for your project
dn.configure(project="airt-image-attack-example")

# Step 1: Define a task that calls your image classification model or API.
# This is a placeholder for a real API call.
@dn.task
async def classify_image(image: dn.Image) -> dict:
    # In a real scenario, this would call your model and return its predictions.
    # For this example, we'll simulate a simple response.
    if image.to_numpy().mean() > 0.5: # A simple check to simulate classification change
      return {"predictions": [{"label": "Granny Smith", "confidence": 0.95}]}
    return {"predictions": [{"label": "Cat", "confidence": 0.98}]}

# Step 2: Wrap the task in a CustomTarget.
target = CustomTarget(task=classify_image)

# Step 3: Configure the Attack.
source_image = dn.Image("path/to/your/cat_image.png") # Replace with a real image path

attack = Attack(
    name="image-misclassification-attack",
    target=target,
    search_strategy=simba_search(source_image, theta=0.05),
    objectives={
        # Objective 1: Maximize the confidence of the wrong label.
        "is_granny_smith": json_path('$.predictions[?(@.label == "Granny Smith")].confidence'),

        # Objective 2: Minimize the visual difference from the original image.
        "l2_distance": image_distance(source_image).bind(dn.TaskInput("image"))
    },
    # The directions must match the objectives above.
    directions=["maximize", "minimize"],
    max_trials=500
)

# Step 4: Run the attack.
results = await attack.console()

best_trial = results.best_trial
if best_trial:
    print(f"Attack finished! Best score: {best_trial.score:.2f}")
    # You can now save or inspect the successful adversarial image:
    # best_trial.candidate.to_pil().save("adversarial_image.png")
```

This example configures an `Attack` that is guided by two competing goals: forcing the model's output towards "Granny Smith" while keeping the generated image as close as possible to the original `source_image`.

## How Image Attacks Work

Attacking image models involves a few key components that differ from generative text attacks.

### Search Strategy: `simba_search`

The **`simba_search`** strategy implements the Simple Black-box Attack algorithm. It's an effective and straightforward method for finding adversarial examples when you have access to the model's confidence scores.

It works by iteratively:
1.  Generating a small, random noise pattern.
2.  Adding that noise to a single pixel or region of the current image.
3.  Querying the `Target` with the perturbed image.
4.  If the model's confidence in the incorrect class increases, the change is kept. Otherwise, it's discarded.

This hill-climbing approach gradually modifies the image until it successfully fools the model.

### Objectives for Images

A successful image attack is a balancing act. You need to both fool the model and ensure the changes are not easily detectable. This is typically modeled with two competing objectives.

1.  **Fooling the Model**: You need a scorer to parse the model's output and extract the confidence score for the target (incorrect) class. The `scorers.json_path` scorer is perfect for this, as it can query nested JSON responses from an API.

2.  **Staying Imperceptible**: You also need a scorer to measure how much the adversarial image has deviated from the original. The `scorers.image_distance` scorer calculates the mathematical distance between two images. By setting its direction to `minimize`, you guide the search to find solutions that are visually close to the source.

<Note>
The `image_distance` scorer uses a special pattern: `.bind(dn.TaskInput("image"))`. This tells the scorer to compare the original `source_image` against the image being passed *into* the `classify_image` task, rather than the task's output. This `bind` mechanism is the standard way to make a scorer evaluate the input of a task.
</Note>

## Advanced: Decision-Based Attacks

Sometimes, a model's API won't return detailed confidence scores. It might only return the final predicted label (e.g., `"Cat"`). In these "decision-based" scenarios, a score-guided search like `simba_search` will not work.

For these cases, you can use **`hop_skip_jump_search`**. It's a more advanced algorithm that works by only needing a binary, "yes" or "no" signal from the model. It estimates the decision boundary of the model and iteratively refines the image to cross it. It is more complex but can succeed where score-based methods fail.