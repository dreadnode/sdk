# Configuration and Context

Transform your tasks, scorers, and agents into CLI-configurable, dynamically reconfigurable components without restructuring your code.

The meta system solves a fundamental problem in AI workflows: you want your components to be configurable and context-aware, but you don't want to sacrifice simplicity or type safety. With two simple primitives—`Config` for configurable parameters and context injection for runtime data access—you can build sophisticated, flexible AI systems that automatically generate CLIs and support dynamic reconfiguration.

## Configuration

### Basic Usage

Mark any parameter in your tasks or scorers with `Config()` to make it configurable via CLI and dynamically reconfigurable:

```python
import dreadnode as dn

@dn.task
def analyze_sentiment(
    text: str,  # Regular input parameter
    model: str = dn.Config("gpt-4", help="LLM model for analysis"),
    temperature: float = dn.Config(0.3, ge=0, le=2, help="Sampling temperature"),
    system_prompt: str = dn.Config("Analyze sentiment", help="System prompt")
) -> str:
    # Your task implementation
    return f"Sentiment analysis using {model} at {temperature}"
```

This configuration becomes available when the task is used as part of a larger system - for example, as a tool in an agent or as part of an evaluation pipeline.

The same pattern works for scorers:

```python
@dn.scorer
def similarity_scorer(
    generated: str,  # Main scorer input
    similarity_method: str = dn.Config("cosine", help="Similarity calculation method"),
    threshold: float = dn.Config(0.7, ge=0, le=1, help="Similarity threshold")
) -> float:
    # Scorer implementation
    return calculate_similarity(generated, method=similarity_method)
```

### Configuration Options

`Config` supports all Pydantic validation options to ensure your parameters are valid:

```python
@dn.task
def text_classification(
    input_text: str,
    # String validation
    model: str = dn.Config("gpt-4", pattern=r"^(gpt-4|claude-3|gpt-4o)$", help="Supported models"),

    # Numeric constraints
    confidence_threshold: float = dn.Config(0.8, ge=0.0, le=1.0, help="Minimum confidence"),
    max_tokens: int = dn.Config(1000, gt=0, le=4000, help="Maximum output tokens"),

    # String constraints
    output_format: str = dn.Config("json", min_length=1, help="Output format"),

    # Required parameters (no default value)
    api_key: str = dn.Config(..., min_length=10, help="Required API key")
) -> str:
    # Implementation
    pass
```

### Annotation Style

You can use Python's `Annotated` syntax to solve two common problems:

**Problem 1: Required parameters that look optional**

```python
from typing import Annotated

@dn.task
def requires_api_key(
    query: str,
    # This looks optional because it has = Config(...), but it's actually required
    api_key: Annotated[str, dn.Config(help="Required API key")] = dn.Config(...),
    # Optional parameter - truly has a default
    model: str = dn.Config("gpt-4", help="Model to use")
) -> str:
    return f"Query: {query} with {model}"
```

**Problem 2: Reusing configuration across multiple components**

```python
# Define reusable configuration types
ModelConfig = Annotated[str, dn.Config(help="LLM model", pattern=r"^(gpt-4|claude-3)$")]
TempConfig = Annotated[float, dn.Config(help="Temperature", ge=0, le=2)]

@dn.task
def task_a(
    input: str,
    model: ModelConfig = "gpt-4",  # Reuse configuration
    temperature: TempConfig = 0.7
) -> str:
    pass

@dn.task
def task_b(
    input: str,
    model: ModelConfig = "claude-3",  # Same config, different default
    temperature: TempConfig = 0.3
) -> str:
    pass
```

!!! note
    This follows the same pattern as Pydantic's `Annotated` usage - the annotation carries the field configuration while the default value provides the actual default.

### Agent Configuration

Agents are the primary configurable entities exposed through the CLI. Agent configuration includes its model, instructions, tools, and other parameters:

```python
# Agent with configurable parameters
agent = dn.Agent(
    name="content_analyzer",
    model=dn.Config("gpt-4", help="LLM model for the agent"),
    instructions=dn.Config("You are a content analysis expert", help="Agent instructions"),
    max_steps=dn.Config(10, help="Maximum conversation steps"),
    tools=[sentiment_analyzer_task, text_classifier_task]  # Tasks become tools
)
```

Users configure the agent via CLI:

```bash
# Configure agent parameters
dreadnode agent run content_analyzer --model="claude-3" --max-steps=20 --instructions="Be concise"

# Get help for all agent configuration options
dreadnode agent run content_analyzer help
```

### CLI Integration

The magic happens automatically. Every `Config` parameter in your agent becomes a CLI option with proper help text, validation, and type conversion:

```bash
# Get help for all configurable parameters
dreadnode agent run my_agent help

# Use configuration files
dreadnode agent run my_agent --config=config.yaml

# Override specific parameters
dreadnode agent run my_agent --config=config.yaml --model="gpt-4o"
```

Configuration files support YAML, JSON, and TOML:

```yaml
# agent_config.yaml
model: "claude-3"
instructions: "You are an expert analyst"
max_steps: 15
```

!!! note
    Configuration validation happens automatically. Invalid values are caught before your task runs, with clear error messages about what went wrong.

## Context Injection

### Basic Usage

Context injection lets you access runtime data anywhere in your component tree without manually passing parameters around. This is especially powerful for scorers that need access to dataset fields or task outputs:

```python
@dn.scorer
def accuracy_scorer(
    prediction: str,  # Task output (automatically provided)
    # Automatically inject from dataset
    ground_truth: str = dn.DatasetField("expected_output"),
    category: str = dn.DatasetField("category", default="general")
) -> float:
    # No manual parameter passing needed
    return 1.0 if prediction == ground_truth else 0.0
```

Your dataset just needs to have the expected fields:

```python
dataset = [
    {"input": "What is AI?", "expected_output": "Artificial Intelligence", "category": "tech"},
    {"input": "Define ML", "expected_output": "Machine Learning", "category": "tech"},
    # scorer automatically gets ground_truth and category from each row
]
```

### Context Types

#### Dataset Context

Access any field from your evaluation dataset:

```python
@dn.scorer
def context_aware_scorer(
    response: str,
    expected: str = dn.DatasetField("expected_output"),
    difficulty: int = dn.DatasetField("difficulty", default=1),
    metadata: dict = dn.DatasetField("metadata", default={})
) -> float:
    # Scoring logic can use all dataset context
    base_score = calculate_similarity(response, expected)
    return base_score * (2.0 if difficulty > 3 else 1.0)
```

#### Task Context

Access inputs and outputs from the current task:

```python
@dn.scorer
def task_context_scorer(
    final_output: str,  # Main scorer input
    # Access the original task input
    original_query: str = dn.TaskInput("query"),
    # Access other task outputs if task returns multiple values
    confidence: float = dn.TaskOutput("confidence", default=1.0)
) -> float:
    # Score based on both input and output context
    relevance = check_relevance(original_query, final_output)
    return relevance * confidence
```

#### Run Parameters

Access configuration from the current evaluation run:

```python
@dn.scorer
def model_aware_scorer(
    text: str,
    # Access run-level configuration
    model_name: str = dn.RunParam("model"),
    experiment_id: str = dn.RunParam("experiment_id", default="default")
) -> float:
    # Adjust scoring based on which model generated the text
    baseline_threshold = 0.7 if model_name == "gpt-4" else 0.6
    return calculate_score(text, threshold=baseline_threshold)
```

#### Execution Context

Access the current execution environment for logging or advanced processing:

```python
@dn.scorer
def execution_aware_scorer(
    response: str,
    # Access current execution spans for logging/tracing
    current_task = dn.CurrentTask(),
    current_run = dn.CurrentRun()
) -> float:
    # Log to current task span
    current_task.log_metric("response_length", len(response))

    # Access run metadata
    run_tags = current_run.tags

    return score_response(response)
```

### Context with Defaults

Always provide sensible defaults for context that might not be available:

```python
@dn.scorer
def robust_scorer(
    prediction: str,
    # Required dataset field
    ground_truth: str = dn.DatasetField("expected"),
    # Optional dataset fields with defaults
    weight: float = dn.DatasetField("sample_weight", default=1.0),
    tags: list = dn.DatasetField("tags", default=[]),
    # Optional run context
    model_version: str = dn.RunParam("model_version", default="unknown")
) -> float:
    base_score = calculate_accuracy(prediction, ground_truth)

    # Apply sample weighting if available
    weighted_score = base_score * weight

    # Adjust for model version if known
    if "v2" in model_version:
        weighted_score *= 1.1  # v2 models get slight boost

    return weighted_score
```

!!! warning
    Context resolution happens at runtime. If a required context field is missing and no default is provided, your scorer will fail with a clear error message indicating what's missing.

### Multi-Source Context

You can mix different context sources in a single component:

```python
@dn.scorer
def comprehensive_scorer(
    response: str,  # Task output

    # Dataset context
    expected: str = dn.DatasetField("expected_output"),
    topic: str = dn.DatasetField("topic", default="general"),

    # Task context
    user_query: str = dn.TaskInput("query"),
    model_confidence: float = dn.TaskOutput("confidence", default=0.5),

    # Run context
    evaluation_mode: str = dn.RunParam("mode", default="standard"),

    # Configuration
    strictness: float = dn.Config(0.8, help="Scoring strictness")
) -> float:
    # Rich scoring logic with access to all context
    pass
```

## Advanced Patterns

### Optimization Studies

During optimization, your task configuration gets automatically reconfigured with different parameter combinations:

```python
from dreadnode.optimization import Study

# Your configurable task
@dn.task
def tunable_task(
    query: str,
    model: str = dn.Config("gpt-4", help="Model to use"),
    temperature: float = dn.Config(0.7, help="Sampling temperature"),
    max_tokens: int = dn.Config(1000, help="Maximum tokens")
) -> str:
    # Task implementation
    pass

# Study automatically tries different configurations
study = Study(
    task=tunable_task,
    scorers=[accuracy_scorer, speed_scorer],

    # Parameters to optimize
    search_space={
        "model": ["gpt-4", "claude-3", "gpt-4o"],
        "temperature": (0.1, 1.0),  # Range for continuous values
        "max_tokens": [500, 1000, 1500, 2000]  # Discrete choices
    }
)

results = study.optimize(dataset, trials=50)
```

Your scorers can access information about the current optimization trial:

```python
@dn.scorer
def trial_aware_scorer(
    output: str,
    expected: str = dn.DatasetField("expected"),
    # Access current trial parameters
    current_trial = dn.CurrentTrial()
) -> float:
    base_score = calculate_accuracy(output, expected)

    # Access what's being tested this trial
    trial_params = current_trial.candidate
    model_being_tested = trial_params.get("model", "unknown")

    # Adjust scoring based on model expectations
    if model_being_tested == "gpt-4":
        return base_score * 1.1  # Higher expectations for GPT-4
    else:
        return base_score
```

### Complex Configuration Hierarchies

For sophisticated agents, you can create nested configuration structures:

```python
@dn.task
def multi_step_analysis(
    input_text: str,

    # Primary model configuration
    primary_model: str = dn.Config("gpt-4", help="Primary analysis model"),
    primary_temperature: float = dn.Config(0.3, help="Primary model temperature"),

    # Fallback configuration
    use_fallback: bool = dn.Config(True, help="Use fallback model if primary fails"),
    fallback_model: str = dn.Config("claude-3", help="Fallback model"),
    fallback_temperature: float = dn.Config(0.7, help="Fallback temperature"),

    # Processing configuration
    max_retries: int = dn.Config(3, gt=0, help="Maximum retry attempts"),
    timeout_seconds: int = dn.Config(30, gt=0, help="Request timeout")
) -> str:
    # Implementation with rich configuration
    pass
```

### Custom Context Sources

For specialized scenarios, you can create custom context providers:

```python
class DatabaseContext(dn.Context):
    def __init__(self, query: str, **kwargs):
        super().__init__(**kwargs)
        self.query = query

    def resolve(self) -> dict:
        # Custom resolution logic
        return fetch_from_database(self.query)

@dn.scorer
def database_enhanced_scorer(
    prediction: str,
    expected: str = dn.DatasetField("expected"),
    # Custom context from external database
    reference_data = DatabaseContext(
        query="SELECT reference FROM knowledge_base WHERE topic=?",
        default={}
    )
) -> float:
    # Scoring with external knowledge
    enhanced_score = calculate_with_reference(prediction, expected, reference_data)
    return enhanced_score
```

### Conditional Configuration

Handle different modes or strategies with conditional configuration:

```python
@dn.task
def adaptive_summarization(
    text: str,

    # Mode selection
    mode: str = dn.Config("balanced", help="Summarization mode: fast|balanced|thorough"),

    # Mode-specific configuration
    fast_model: str = dn.Config("gpt-3.5-turbo", help="Model for fast mode"),
    balanced_model: str = dn.Config("gpt-4", help="Model for balanced mode"),
    thorough_model: str = dn.Config("gpt-4", help="Model for thorough mode"),

    # Shared configuration
    max_summary_length: int = dn.Config(200, help="Maximum summary length")
) -> str:
    # Select model based on mode
    model_map = {
        "fast": fast_model,
        "balanced": balanced_model,
        "thorough": thorough_model
    }

    selected_model = model_map.get(mode, balanced_model)
    # Implementation uses selected_model and max_summary_length
    pass
```

### Cross-Component Configuration Sharing

Share configuration across multiple components to keep them synchronized:

```python
# Shared configuration definitions
shared_model_config = dn.Config("gpt-4", help="Model used across all components")
shared_temperature_config = dn.Config(0.7, help="Temperature used across all components")

@dn.task
def preprocessing_task(
    text: str,
    model: str = shared_model_config,  # Reuse configuration
    temperature: float = shared_temperature_config,
    chunk_size: int = dn.Config(1000, help="Text chunk size")
) -> list[str]:
    # Preprocessing implementation
    pass

@dn.task
def analysis_task(
    chunks: list[str],
    model: str = shared_model_config,  # Same configuration
    temperature: float = shared_temperature_config,
    analysis_depth: str = dn.Config("thorough", help="Analysis thoroughness")
) -> dict:
    # Analysis implementation
    pass
```

This ensures consistency when both tasks are used as tools within the same agent or evaluation pipeline.

!!! tip
    Shared configuration is especially useful for maintaining consistency across complex multi-task agents where you want certain parameters (like model choice) to be synchronized across all components.

## Common Patterns and Best Practices

### Configuration Design

**Provide meaningful defaults and help text:**

```python
@dn.task
def well_configured_task(
    input_text: str,
    # Good: descriptive help and sensible default
    model: str = dn.Config("gpt-4", help="LLM model - gpt-4 for quality, gpt-3.5-turbo for speed"),
    # Good: validation with business context
    confidence_threshold: float = dn.Config(0.8, ge=0.5, le=1.0, help="Minimum confidence (0.8 recommended for production)")
) -> str:
    pass
```

**Group related configuration logically:**

```python
@dn.task
def organized_task(
    query: str,

    # Model configuration group
    model: str = dn.Config("gpt-4", help="Primary LLM model"),
    temperature: float = dn.Config(0.7, help="Model temperature"),
    max_tokens: int = dn.Config(1000, help="Maximum output tokens"),

    # Processing configuration group
    batch_size: int = dn.Config(10, help="Processing batch size"),
    parallel_requests: int = dn.Config(3, help="Concurrent API requests"),

    # Output configuration group
    format: str = dn.Config("json", help="Output format"),
    include_metadata: bool = dn.Config(True, help="Include processing metadata")
) -> str:
    pass
```

### Context Usage Best Practices

**Always provide defaults for optional context:**

```python
@dn.scorer
def robust_context_scorer(
    response: str,
    # Required - will fail if missing
    expected: str = dn.DatasetField("expected_output"),
    # Optional with sensible defaults
    difficulty: int = dn.DatasetField("difficulty", default=1),
    category: str = dn.DatasetField("category", default="general"),
    sample_weight: float = dn.DatasetField("weight", default=1.0)
) -> float:
    # Implementation handles all cases gracefully
    pass
```

**Use context for cross-cutting concerns, explicit parameters for core logic:**

```python
@dn.scorer
def well_designed_scorer(
    prediction: str,  # Core input - explicit parameter
    threshold: float = dn.Config(0.8, help="Similarity threshold"),  # Core config - explicit

    # Cross-cutting context - injected
    ground_truth: str = dn.DatasetField("expected"),
    sample_metadata: dict = dn.DatasetField("metadata", default={}),
    current_model: str = dn.RunParam("model", default="unknown")
) -> float:
    # Core logic uses explicit parameters
    similarity = calculate_similarity(prediction, ground_truth, threshold)

    # Cross-cutting concerns use context
    if sample_metadata.get("is_adversarial", False):
        similarity *= 0.9  # Penalize adversarial examples slightly

    return similarity
```

The meta system enables dreadnode's "configuration as code" philosophy—your components become dynamically configurable and context-aware without sacrificing simplicity or type safety. Configuration flows naturally from your code to CLI interfaces, and context injection eliminates parameter threading while maintaining clear dependencies.